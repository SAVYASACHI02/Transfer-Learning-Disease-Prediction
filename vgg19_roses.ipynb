{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fda941e-07af-42a1-a88c-466c16487a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB4\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7808e6a5-61a6-486d-91d5-68b2ab0b64ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(root_dir, output_dir, test_size=0.15, val_size=0.15, seed=42):\n",
    "    \"\"\"\n",
    "    Splits dataset into train/val/test sets and creates directory structure\n",
    "    \"\"\"\n",
    "    # Create output directories\n",
    "    os.makedirs(os.path.join(output_dir, 'train'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'val'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'test'), exist_ok=True)\n",
    "\n",
    "    # Process each class\n",
    "    for class_name in ['Healthy', 'Unhealthy']:\n",
    "        # Create class directories in train/val/test\n",
    "        os.makedirs(os.path.join(output_dir, 'train', class_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_dir, 'val', class_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_dir, 'test', class_name), exist_ok=True)\n",
    "\n",
    "        # Get list of images\n",
    "        class_dir = os.path.join(root_dir, class_name)\n",
    "        images = [f for f in os.listdir(class_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        # Split into temp (85%) and test (15%)\n",
    "        temp_images, test_images = train_test_split(images, \n",
    "                                                  test_size=test_size, \n",
    "                                                  random_state=seed)\n",
    "        \n",
    "        # Split temp into train (82.35%) and val (17.65%) to get final 70-15-15 split\n",
    "        train_images, val_images = train_test_split(temp_images, \n",
    "                                                   test_size=val_size/(1-test_size), \n",
    "                                                   random_state=seed)\n",
    "        \n",
    "        # Function to copy images\n",
    "        def copy_files(file_list, split_name):\n",
    "            for f in file_list:\n",
    "                src = os.path.join(class_dir, f)\n",
    "                dst = os.path.join(output_dir, split_name, class_name, f)\n",
    "                shutil.copyfile(src, dst)\n",
    "        \n",
    "        # Copy files to respective directories\n",
    "        copy_files(train_images, 'train')\n",
    "        copy_files(val_images, 'val')\n",
    "        copy_files(test_images, 'test')\n",
    "\n",
    "# Usage - modify these paths according to your setup\n",
    "input_dir = './Rose'  # Should contain 'diseased' and 'not_diseased' folders\n",
    "output_dir = 'images_rose'  # New directory that will be created\n",
    "\n",
    "split_dataset(input_dir, output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfa0a49-e40c-480c-a9ac-f81543e6bc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2054 images belonging to 2 classes.\n",
      "Found 441 images belonging to 2 classes.\n",
      "Found 441 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kavit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1532s\u001b[0m 24s/step - accuracy: 0.6209 - loss: 0.7899 - val_accuracy: 0.8462 - val_loss: 0.3932\n",
      "Epoch 2/8\n",
      "\u001b[1m 1/64\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21:58\u001b[0m 21s/step - accuracy: 0.7812 - loss: 0.4224"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kavit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 4s/step - accuracy: 0.7812 - loss: 0.4224 - val_accuracy: 0.7861 - val_loss: 0.4079\n",
      "Epoch 3/8\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1428s\u001b[0m 22s/step - accuracy: 0.8031 - loss: 0.4019 - val_accuracy: 0.8966 - val_loss: 0.3046\n",
      "Epoch 4/8\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 2s/step - accuracy: 0.8750 - loss: 0.2571 - val_accuracy: 0.8846 - val_loss: 0.3108\n",
      "Epoch 5/8\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 6s/step - accuracy: 0.8866 - loss: 0.2825 - val_accuracy: 0.8894 - val_loss: 0.2774\n",
      "Epoch 6/8\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 876ms/step - accuracy: 1.0000 - loss: 0.1982 - val_accuracy: 0.8990 - val_loss: 0.2728\n",
      "Epoch 7/8\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m480s\u001b[0m 8s/step - accuracy: 0.9057 - loss: 0.2428 - val_accuracy: 0.9087 - val_loss: 0.2415\n",
      "Epoch 8/8\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 1s/step - accuracy: 0.9688 - loss: 0.2444 - val_accuracy: 0.9038 - val_loss: 0.2442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./saved_model/rose_health_classifier.h5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 8s/step - accuracy: 0.9283 - loss: 0.2310\n",
      "Test Accuracy: 0.9111\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define paths\n",
    "base_dir = './images_rose'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'val')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "# Image preprocessing and augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255  # Normalize pixel values to [0, 1]\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)  # Only rescale for validation\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)  # Only rescale for testing\n",
    "\n",
    "# Create data generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),  # Resize images to 224x224 (VGG19 input size)\n",
    "    batch_size=32,\n",
    "    class_mode='binary'  # Binary classification (healthy/unhealthy)\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    shuffle=False  # Do not shuffle for evaluation\n",
    ")\n",
    "\n",
    "# Load the VGG19 model (pre-trained on ImageNet)\n",
    "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model (do not train the pre-trained layers)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom layers on top of the base model\n",
    "x = base_model.output\n",
    "x = Flatten()(x)  # Flatten the output of the base model\n",
    "x = Dense(512, activation='relu')(x)  # Add a fully connected layer\n",
    "x = Dropout(0.5)(x)  # Add dropout for regularization\n",
    "predictions = Dense(1, activation='sigmoid')(x)  # Output layer for binary classification\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    epochs=8  # Number of epochs\n",
    ")\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = './saved_model/rose_health_classifier.h5'  # Path to save the model\n",
    "os.makedirs(os.path.dirname(model_save_path), exist_ok=True)  # Ensure the directory exists\n",
    "model.save(model_save_path)  # Save the model\n",
    "print(f'Model saved to {model_save_path}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_generator, steps=test_generator.samples // test_generator.batch_size)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = model.predict(test_generator)\n",
    "y_pred = np.round(y_pred).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Classification report and confusion matrix\n",
    "print('Classification Report:')\n",
    "print(classification_report(test_generator.classes, y_pred))\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(test_generator.classes, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28127f46-4ec7-4f34-ba53-00f4a1ac3123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
